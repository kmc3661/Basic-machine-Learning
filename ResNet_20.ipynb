{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_20.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmc3661/Basic-machine-Learning/blob/master/ResNet_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owEVRurBjqv3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer,self).__init__()\n",
        "        self.lambd=lambd\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.lambd\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion=1\n",
        "\n",
        "    def __init__(self,in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__() #부모 클래스를 먼저 실행시켜서 적용되기 하기 위함\n",
        "        #nn.module의 함수들을 가져와서 사용하기 위해\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes,kernel_size=3, stride=stride,padding=1,bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes,planes,kernel_size=3,stride=1, padding=1,bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential() #identity, stride=1 일때\n",
        "        if stride !=1 or in_planes != planes: #identity mapping이 아닌경우\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "            \"\"\"\n",
        "            self.shortcut = LambdaLayer(lambda x: \n",
        "                                       F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4, \"constant\", 0))\n",
        "            )\n",
        "            \"\"\"\n",
        "\n",
        "    def forward(self,x):\n",
        "        out= F.relu(self.bn1(self.conv1(x)))\n",
        "        out=self.bn2(self.conv2(out))\n",
        "        out+=self.shortcut(x)\n",
        "        out=F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10): #block: basic block\n",
        "        super(ResNet,self).__init__()\n",
        "        self.in_planes=16 # CIFAR-10에 맞춰서 imageNet의 크기보다 줄여서 학습\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1) # layer 한개당 basick block 3개(Convlayer 6개)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes) # FCL\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1) # 리스트 +리스트는 연산이 아닌 가로로 concatenate\n",
        "        #[1]*n은 [1,1,1,1 -> n개]로 만들어줌\n",
        "        #strides=[1,1,1] in layer1,[2,1,1] in 2,[2,1,1] in 3\n",
        "        #첫번째 conv연산에서만 downsampling해주기 위함\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride)) #Basic block이 추가됨\n",
        "            self.in_planes = planes # 다음 레이어를 위한 채널 수 변경\n",
        "\n",
        "        return nn.Sequential(*layers)# 차원 한개를 없앰\n",
        "\n",
        "    def forward(self, x):\n",
        "        out= F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0),-1) # view: reshape역할, 텐서의 차원을 한 단계 낮추는 역할\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet20():\n",
        "    return ResNet(BasicBlock,[3,3,3]) # block이 세번씩 반복되도록\n",
        "\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset download and load"
      ],
      "metadata": {
        "id": "tp-GvVfu6-2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "valid_size=0.1\n",
        "shuffle = True\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456,0.456], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "     transforms.RandomCrop(32, padding=4),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),  #Numpy 배열의 이미지를 torch 텐서로 바꿔줌   , swap axes  (H*W*C -> C*H*W) \n",
        "     normalize,                       \n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='/data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\"\"\"\n",
        "num_train=len(train_dataset)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "\"\"\"\n",
        "torch.manual_seed(43)\n",
        "val_size = 5000\n",
        "train_size =len(train_dataset) - val_size\n",
        "train_ds, val_ds = random_split(train_dataset,[train_size, val_size])\n",
        "\n",
        "print(len(train_ds))\n",
        "print(len(val_ds))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128,shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=100,shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "CaYLYSeVACqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "writer = SummaryWriter('runs/ResNet_50')\n",
        "device = 'cuda'\n",
        "\n",
        "net= ResNet20()\n",
        "net = net.to(device) #모델을 GPU에 넣기\n",
        "net = torch.nn.DataParallel(net) #모델을 병렬로 실행하여 다수의 GPU에서 작업\n",
        "cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'resnet20_cifar10.pt'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate,momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\n[ Train epoch:%d ]'%epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct =0\n",
        "    total =0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader): #batch_size만큼 데이터를 뽑아냄\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()# 한번의 학습이 완료될 때마다 gradient를 0으로 초기화\n",
        "\n",
        "        benign_outputs = net(inputs)\n",
        "        loss = criterion(benign_outputs, targets) #benign_outputs: 예측된 값, traget: GT\n",
        "        loss.backward() #backpropagation\n",
        "\n",
        "        optimizer.step()#Gradient descent -> 모델 업데이트\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = benign_outputs.max(1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item() #예측한 값이 target과 일치할때 1씩 더해짐\n",
        "        \n",
        "        if batch_idx %100 ==0:\n",
        "            print('\\nCurrent batch:',str(batch_idx))\n",
        "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item()/ targets.size(0)))\n",
        "            print('Current benign train loss:', loss.item())\n",
        "\n",
        "    print('\\nTotal benign train accuracy:', 100. * correct / total) #전체 target중 맞춘 비율\n",
        "    writer.add_scalar(\"train accuracy\", 100. * correct / total,epoch)\n",
        "    print('train_average loss:', train_loss/total)\n",
        "    writer.add_scalar(\"train loss\", train_loss/total,epoch)\n",
        "\n",
        "\n",
        "def validate(epoch):\n",
        "    print('\\n[ val_epoch: %d ]' % epoch)\n",
        "    net.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print('\\nval_accuarcy:', 100. * correct / total)\n",
        "    writer.add_scalar(\"validation accuracy\", 100. * correct / total, epoch)\n",
        "    print('val_average loss:', loss / total)\n",
        "    writer.add_scalar(\"validation loss\", loss/total, epoch)\n",
        "\n",
        "    state = {\n",
        "        'net': net.state_dict()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state,'./checkpoint/'+ file_name)\n",
        "    print('Model Saved!')\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n[ test_epoch: %d ]' % epoch)\n",
        "    net.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print('\\ntest_accuarcy:', 100. * correct / total)\n",
        "    writer.add_scalar(\"test_accuarcy\", 100* correct / total, epoch)\n",
        "    print('test_average loss:', loss / total)\n",
        "    writer.add_scalar(\"test_loss\",loss/total,epoch)\n",
        "\n",
        "    state = {\n",
        "        'net': net.state_dict()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state,'./checkpoint/'+ file_name)\n",
        "    print('Model Saved!')\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr= learning_rate\n",
        "    if epoch >= 100:\n",
        "            lr /= 10\n",
        "    if epoch >= 150:\n",
        "            lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "for epoch in range(200):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "\n",
        "writer.flush() #버퍼의 내용을 파일에 기록\n",
        "writer.close() #stream을 종료"
      ],
      "metadata": {
        "id": "QVe4gkGa-ePj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboard --upgrade"
      ],
      "metadata": {
        "id": "nuBsp1zDLK6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n"
      ],
      "metadata": {
        "id": "QdJUH0AOLNwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "YOxOs93bKFPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}